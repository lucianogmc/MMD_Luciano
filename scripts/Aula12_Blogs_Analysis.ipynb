{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mestrado em Modelagem Matematica da Informacao\n",
      "\n",
      "Master Program - Mathematical Modeling of Information\n",
      "\n",
      "Disciplina: Modelagem e Mineracao de Dados\n",
      "\n",
      "Course: Data Mining and Modeling\n",
      "\n",
      "Professor: Renato Rocha Souza\n",
      "\n",
      "Topic: Blogs Analysis (class #11)\n",
      "\n",
      "Code adapted from:\n",
      "\n",
      "    blogs_and_nlp__get_feed.py,\n",
      "\n",
      "    blogs_and_nlp__sentence_detection.py,\n",
      "\n",
      "    blogs_and_nlp__summarize.py,\n",
      "\n",
      "    blogs_and_nlp__summarize_markedup_output.py,\n",
      "\n",
      "    blogs_and_nlp__extract_entities.py,\n",
      "    \n",
      "    blogs_and_nlp__extract_interactions.py\n",
      "    \n",
      "    blogs_and_nlp__extract_interactions_markedup_output.py\n",
      "\n",
      "Information on the Python Packages used:\n",
      "\n",
      "http://docs.python.org/library/os\n",
      "\n",
      "http://docs.python.org/library/sys.html\n",
      "\n",
      "http://docs.python.org/library/datetime.html\n",
      "\n",
      "http://docs.python.org/library/json.html\n",
      "\n",
      "http://code.google.com/p/feedparser/\n",
      "\n",
      "http://numpy.scipy.org/\n",
      "\n",
      "http://www.crummy.com/software/BeautifulSoup/\n",
      "\n",
      "http://nltk.org/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "from datetime import datetime as dt\n",
      "import json\n",
      "import codecs\n",
      "import feedparser\n",
      "import numpy\n",
      "from BeautifulSoup import BeautifulStoneSoup\n",
      "import nltk \n",
      "from nltk.tokenize import word_tokenize "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specifying the path to the files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datapath = \"/home/rsouza/Dropbox/Renato/ModMinDados/Git/datasets/\"\n",
      "templates = \"/home/rsouza/Dropbox/Renato/ModMinDados/Git/templates/\"\n",
      "outputs = \"/home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/\"\n",
      "\n",
      "lfeeds = 'ch05_feedlist_en.txt'\n",
      "jfile = 'blogfeeds.json'\n",
      "\n",
      "feed_urls = (datapath+lfeeds)\n",
      "jsonfile = (outputs+jfile)\n",
      "\n",
      "feed_url = 'http://feeds2.feedburner.com/zdnet/hardware'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Choosing the list of stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist_en = nltk.corpus.stopwords.words('english')\n",
      "stoplist_pt = nltk.corpus.stopwords.words('portuguese')\n",
      "ignore_signs = [\".\",\",\",\"--\",\"'s\",\"?\",\")\",\"(\",\":\",\"'\",\"'re\",'\"',\"-\",\"}\",\"{\",'']\n",
      "ignorelist = stoplist_en + stoplist_pt + ignore_signs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML_TEMPLATE = u\"\"\"<html>\n",
      "    <head>\n",
      "        <title>{}</title>\n",
      "        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/>\n",
      "    </head>\n",
      "    <body>{}</body>\n",
      "</html>\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cleanHtml(html):\n",
      "    return BeautifulStoneSoup(nltk.clean_html(html), convertEntities=BeautifulStoneSoup.HTML_ENTITIES).contents[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_feed(feed):\n",
      "    fp = feedparser.parse(feed.encode('utf-8'))\n",
      "    print \"{} entries retrieved from feed '{}'\".format(len(fp.entries[0].title), fp.feed.title)\n",
      "    blog_posts = []\n",
      "    for e in fp.entries:\n",
      "        blog_posts.append({'title': e.title, 'content':cleanHtml(e.content[0].value), 'link':e.links[0].href})\n",
      "    f = open(jsonfile, 'w')\n",
      "    f.write(json.dumps(blog_posts))\n",
      "    f.close()\n",
      "    print >> sys.stderr, u'Content saved in: {}'.format(f.name, )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sentence_detection():\n",
      "    blog_data = json.loads(codecs.open(jsonfile, encoding=\"utf-8\").read())    \n",
      "    for post in blog_data:\n",
      "        sentences = nltk.tokenize.sent_tokenize(post['content'])\n",
      "        words = [w.lower() for sentence in sentences for w in\n",
      "                 nltk.tokenize.word_tokenize(sentence)]\n",
      "        fdist = nltk.FreqDist(words)\n",
      "        # Basic stats\n",
      "        num_words = sum([i[1] for i in fdist.items()])\n",
      "        num_unique_words = len(fdist.keys())\n",
      "        # Hapaxes are words that appear only once\n",
      "        num_hapaxes = len(fdist.hapaxes())\n",
      "        top_10_words_sans_stop_words = [w for w in fdist.items() if w[0] not in ignorelist and len(w[0]) > 2][:10]\n",
      "        print post['title']\n",
      "        print u'\\tNum Sentences:'.ljust(25), len(sentences)\n",
      "        print u'\\tNum Words:'.ljust(25), num_words\n",
      "        print u'\\tNum Unique Words:'.ljust(25), num_unique_words\n",
      "        print u'\\tNum Hapaxes:'.ljust(25), num_hapaxes\n",
      "        print u'\\tTop 10 Most Frequent Words (sans stop words):\\n\\t\\t', \\\n",
      "                u'\\n\\t\\t'.join([u'{} ({})'.format(w[0], w[1]) for w in top_10_words_sans_stop_words])\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def score_sentences(sentences, important_words):\n",
      "    # Approach taken from \"The Automatic Creation of Literature Abstracts\" by H.P. Luhn\n",
      "    CLUSTER_THRESHOLD = 5  # Distance between words to consider\n",
      "    scores = []\n",
      "    sentence_idx = -1\n",
      "    for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
      "        sentence_idx += 1\n",
      "        word_idx = []\n",
      "        # For each word in the word list...\n",
      "        for w in important_words:\n",
      "            try:\n",
      "                # Compute an index for where any important words occur in the sentence\n",
      "                word_idx.append(s.index(w))\n",
      "            except ValueError, e: # w not in this particular sentence\n",
      "                pass\n",
      "        word_idx.sort()\n",
      "        # It is possible that some sentences may not contain any important words at all\n",
      "        if len(word_idx)== 0: continue\n",
      "        # Using the word index, compute clusters by using a max distance threshold\n",
      "        # for any two consecutive words\n",
      "        clusters = []\n",
      "        cluster = [word_idx[0]]\n",
      "        i = 1\n",
      "        while i < len(word_idx):\n",
      "            if word_idx[i] - word_idx[i - 1] < CLUSTER_THRESHOLD:\n",
      "                cluster.append(word_idx[i])\n",
      "            else:\n",
      "                clusters.append(cluster[:])\n",
      "                cluster = [word_idx[i]]\n",
      "            i += 1\n",
      "        clusters.append(cluster)\n",
      "        # Score each cluster. The max score for any given cluster is the score \n",
      "        # for the sentence\n",
      "        max_cluster_score = 0\n",
      "        for c in clusters:\n",
      "            significant_words_in_cluster = len(c)\n",
      "            total_words_in_cluster = c[-1] - c[0] + 1\n",
      "            score = 1.0 * significant_words_in_cluster \\\n",
      "                * significant_words_in_cluster / total_words_in_cluster\n",
      "            if score > max_cluster_score:\n",
      "                max_cluster_score = score\n",
      "        scores.append((sentence_idx, score))\n",
      "    return scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def summarize(txt):\n",
      "    TOP_SENTENCES = 5  # Number of sentences to choose on \"top n\"\n",
      "    N = 100  # Number of words to consider\n",
      "    sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
      "    normalized_sentences = [s.lower() for s in sentences]\n",
      "    words = [w.lower() for sentence in normalized_sentences for w in nltk.tokenize.word_tokenize(sentence)]\n",
      "    fdist = nltk.FreqDist(words)\n",
      "    top_n_words = [w[0] for w in fdist.items() if w[0] not in ignorelist][:N]\n",
      "    scored_sentences = score_sentences(normalized_sentences, top_n_words)\n",
      "    # First approach:\n",
      "    # Filter out non-significant sentences by using the average score plus a\n",
      "    # fraction of the std dev as a filter\n",
      "    avg = numpy.mean([s[1] for s in scored_sentences])\n",
      "    std = numpy.std([s[1] for s in scored_sentences])\n",
      "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences\n",
      "                   if score > avg + 0.5 * std]\n",
      "    # Second Approach: \n",
      "    # Return only the top N ranked sentences\n",
      "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-TOP_SENTENCES:]\n",
      "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
      "    # Decorate the post object with summaries\n",
      "    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],\n",
      "                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_summaries():\n",
      "    blog_data = json.loads(codecs.open(jsonfile, encoding=\"utf-8\").read())    \n",
      "    for post in blog_data:\n",
      "        post.update(summarize(post['content']))\n",
      "        print post['title']\n",
      "        print u'-' * len(post['title'])\n",
      "        print\n",
      "        print '-------------'\n",
      "        print 'Top N Summary'\n",
      "        print '-------------'\n",
      "        print u' '.join(post['top_n_summary'])\n",
      "        print\n",
      "        print '-------------------'\n",
      "        print 'Mean Scored Summary'\n",
      "        print '-------------------'\n",
      "        print u' '.join(post['mean_scored_summary'])\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_html_summaries():\n",
      "    blog_data = json.loads(open(jsonfile).read())    \n",
      "    for post in blog_data:\n",
      "        post.update(summarize(post['content']))\n",
      "        # You could also store a version of the full post with key sentences markedup\n",
      "        # for analysis with simple string replacement...\n",
      "        for summary_type in ['top_n_summary', 'mean_scored_summary']:\n",
      "            post[summary_type + '_marked_up'] = u'<p>{}</p>'.format(post['content'], )\n",
      "            for s in post[summary_type]:\n",
      "                post[summary_type + '_marked_up'] = \\\n",
      "                post[summary_type + '_marked_up'].replace(s, u'<strong>{}</strong>'.format(s, ))\n",
      "            filename = post['title'] + '.summary.' + summary_type + '.html'\n",
      "            filename = filename.replace('/','-')\n",
      "            f = open(outputs+filename, 'w')\n",
      "            html = HTML_TEMPLATE.format(post['title'] + ' Summary', post[summary_type + '_marked_up'],)\n",
      "            f.write(html.encode('utf-8'))\n",
      "            f.close()\n",
      "            print >> sys.stderr, \"Content saved in: \", f.name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_entities():\n",
      "    blog_data = json.loads(codecs.open(jsonfile, encoding=\"utf-8\").read())\n",
      "    for post in blog_data:\n",
      "        sentences = nltk.tokenize.sent_tokenize(post['content'])\n",
      "        tokens = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
      "        pos_tagged_tokens = [nltk.pos_tag(t) for t in tokens]\n",
      "        # Flatten the list since we're not using sentence structure\n",
      "        # and sentences are guaranteed to be separated by a special\n",
      "        # POS tuple such as ('.', '.')\n",
      "        pos_tagged_tokens = [token for sent in pos_tagged_tokens for token in sent]\n",
      "        all_entity_chunks = []\n",
      "        previous_pos = None\n",
      "        current_entity_chunk = []\n",
      "        for (token, pos) in pos_tagged_tokens:\n",
      "            if pos == previous_pos and pos.startswith('NN'):\n",
      "                current_entity_chunk.append(token)\n",
      "            elif pos.startswith('NN'):\n",
      "                if current_entity_chunk != []:\n",
      "                    # Note that current_entity_chunk could be a duplicate when appended,\n",
      "                    # so frequency analysis again becomes a consideration\n",
      "                     all_entity_chunks.append((' '.join(current_entity_chunk), pos))\n",
      "                current_entity_chunk = [token]\n",
      "            previous_pos = pos\n",
      "        # Store the chunks as an index for the document\n",
      "        # and account for frequency while we're at it...\n",
      "        post['entities'] = {}\n",
      "        for c in all_entity_chunks:\n",
      "            post['entities'][c] = post['entities'].get(c, 0) + 1\n",
      "        # For example, we could display just the title-cased entities\n",
      "        print post['title']\n",
      "        print '-' * len(post['title'])\n",
      "        proper_nouns = []\n",
      "        for (entity, pos) in post['entities']:\n",
      "            if entity.istitle():\n",
      "                print u'\\t{} ({})'.format(entity, post['entities'][(entity, pos)])\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_interactions(txt):\n",
      "    sentences = nltk.tokenize.sent_tokenize(txt)\n",
      "    tokens = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
      "    pos_tagged_tokens = [nltk.pos_tag(t) for t in tokens]\n",
      "    entity_interactions = []\n",
      "    for sentence in pos_tagged_tokens:\n",
      "        all_entity_chunks = []\n",
      "        previous_pos = None\n",
      "        current_entity_chunk = []\n",
      "        for (token, pos) in sentence:\n",
      "            if pos == previous_pos and pos.startswith('NN'):\n",
      "                current_entity_chunk.append(token)\n",
      "            elif pos.startswith('NN'):\n",
      "                if current_entity_chunk != []:\n",
      "                    all_entity_chunks.append((' '.join(current_entity_chunk),\n",
      "                            pos))\n",
      "                current_entity_chunk = [token]\n",
      "            previous_pos = pos\n",
      "        if len(all_entity_chunks) > 1:\n",
      "            entity_interactions.append(all_entity_chunks)\n",
      "        else:\n",
      "            entity_interactions.append([])\n",
      "    assert len(entity_interactions) == len(sentences)\n",
      "    return dict(entity_interactions=entity_interactions,\n",
      "                sentences=sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_interactions():\n",
      "    blog_data = json.loads(open(jsonfile).read())\n",
      "    for post in blog_data:\n",
      "        post.update(extract_interactions(post['content']))\n",
      "        print post['title']\n",
      "        print u'-' * len(post['title'])\n",
      "        for interactions in post['entity_interactions']:\n",
      "            print u'; '.join([i[0] for i in interactions])\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_html_interactions():\n",
      "    blog_data = json.loads(codecs.open(jsonfile, encoding=\"utf-8\").read())\n",
      "    for post in blog_data:\n",
      "        post.update(extract_interactions(post['content']))\n",
      "        # Display output as markup with entities presented in bold text\n",
      "        post['markup'] = []\n",
      "        for sentence_idx in range(len(post['sentences'])):\n",
      "            s = post['sentences'][sentence_idx]\n",
      "            for (term, _) in post['entity_interactions'][sentence_idx]:\n",
      "                s = s.replace(term, u'<strong>{}</strong>'.format(term, ))\n",
      "            post['markup'] += [s]\n",
      "        filename = post['title'] + '.interactions.html'\n",
      "        filename = filename.replace('/','-')\n",
      "        f = open(outputs+filename, 'w')\n",
      "        html = HTML_TEMPLATE.format(post['title'] + ' Interactions', ' '.join(post['markup']),)\n",
      "        f.write(html.encode('utf-8'))\n",
      "        f.close()\n",
      "        print >> sys.stderr, u\"Content saved in: {}\".format(f.name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reading the list of blogs to analyze..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feedlist=[line for line in file(feed_urls)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Para executar a lista completa de feeds de uma vez, poder\u00edamos criar o seguinte loop:\n",
      "\n",
      "for i in range(len(feedlist)):\n",
      "    print('Processing feed {} ({}/{})').format(feedlist[i],i+1,len(feedlist)+1)\n",
      "    get_feed(feedlist[i])\n",
      "    sentence_detection()\n",
      "    show_summaries()\n",
      "    save_html_summaries()\n",
      "    extract_entities()\n",
      "    show_interactions()\n",
      "    save_html_interactions()\n",
      "\n",
      "Mas para melhor examinarmos os resultados, vamos executar um feed de cada vez:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_feed(feedlist[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "32 entries retrieved from feed 'O'Reilly Radar - Insight, analysis, and research about emerging technologies'\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/blogfeeds.json\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usando o arquivo json salvo:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence_detection()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Four short links: 28 August 2013\n",
        "\tNum Sentences:           4\n",
        "\tNum Words:               88\n",
        "\tNum Unique Words:        64\n",
        "\tNum Hapaxes:             52\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tlook (2)\n",
        "\t\ttechnology (2)\n",
        "\t\tvia (2)\n",
        "\t\tbig (1)\n",
        "\t\tcanonical\u2019s (1)\n",
        "\t\tchef (1)\n",
        "\t\tcloud (1)\n",
        "\t\tcontrols (1)\n",
        "\t\tcourtney (1)\n",
        "\t\tcultural (1)\n",
        "\n",
        "Four short links: 27 August 2013\n",
        "\tNum Sentences:           8\n",
        "\tNum Words:               172\n",
        "\tNum Unique Words:        113\n",
        "\tNum Hapaxes:             93\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tweb (6)\n",
        "\t\tcracking (2)\n",
        "\t\tflaw (2)\n",
        "\t\tmak\u00ading (2)\n",
        "\t\tpassword (2)\n",
        "\t\tadobe (1)\n",
        "\t\tadopt\u00ading (1)\n",
        "\t\tads (1)\n",
        "\t\tad\u00adver\u00adtis\u00ading (1)\n",
        "\t\talex (1)\n",
        "\n",
        "Four short links: 26 August 2013\n",
        "\tNum Sentences:           5\n",
        "\tNum Words:               91\n",
        "\tNum Unique Words:        65\n",
        "\tNum Hapaxes:             51\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tdrones (3)\n",
        "\t\tinstead (2)\n",
        "\t\tperuvian (2)\n",
        "\t\tvia (2)\n",
        "\t\tarchaeologists (1)\n",
        "\t\tbipedal (1)\n",
        "\t\tbob (1)\n",
        "\t\tbuilding (1)\n",
        "\t\tbull (1)\n",
        "\t\tclient (1)\n",
        "\n",
        "Four short links: 23 August 2013\n",
        "\tNum Sentences:           7\n",
        "\tNum Words:               139\n",
        "\tNum Unique Words:        100\n",
        "\tNum Hapaxes:             79\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tamerica (2)\n",
        "\t\tdata (2)\n",
        "\t\tdrug (2)\n",
        "\t\tdrugs (2)\n",
        "\t\thalf (2)\n",
        "\t\tmining (2)\n",
        "\t\tnew (2)\n",
        "\t\tweka (2)\n",
        "\t\t2012 (1)\n",
        "\t\t5bb (1)\n",
        "\n",
        "Shakespeare and the myth of publishing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tNum Sentences:           59\n",
        "\tNum Words:               1405\n",
        "\tNum Unique Words:        505\n",
        "\tNum Hapaxes:             322\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tplays (12)\n",
        "\t\tbook (11)\n",
        "\t\tit\u2019s (10)\n",
        "\t\tshakespeare (10)\n",
        "\t\ttext (10)\n",
        "\t\tpublishing (9)\n",
        "\t\tfirst (8)\n",
        "\t\tauthor (6)\n",
        "\t\tshakespeare\u2019s (6)\n",
        "\t\taround (5)\n",
        "\n",
        "Four short links: 22 August 2013\n",
        "\tNum Sentences:           7\n",
        "\tNum Words:               194\n",
        "\tNum Unique Words:        137\n",
        "\tNum Hapaxes:             114\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\taround (2)\n",
        "\t\tbletchley (2)\n",
        "\t\tencoding (2)\n",
        "\t\thuxley (2)\n",
        "\t\tinformation (2)\n",
        "\t\tvia (2)\n",
        "\t\t1600 (1)\n",
        "\t\t2013 (1)\n",
        "\t\tago (1)\n",
        "\t\talex (1)\n",
        "\n",
        "Four short links: 21 August 2013\n",
        "\tNum Sentences:           8\n",
        "\tNum Words:               236\n",
        "\tNum Unique Words:        148\n",
        "\tNum Hapaxes:             119\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tqueries (4)\n",
        "\t\tchina (3)\n",
        "\t\tgoogle (3)\n",
        "\t\tttp (3)\n",
        "\t\tblinkdb (2)\n",
        "\t\tforeign (2)\n",
        "\t\tgithub (2)\n",
        "\t\tinvolving (2)\n",
        "\t\tproject (2)\n",
        "\t\tresearchers (2)\n",
        "\n",
        "If This/Then That (IFTTT) and the Belkin WeMo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tNum Sentences:           72\n",
        "\tNum Words:               1830\n",
        "\tNum Unique Words:        573\n",
        "\tNum Hapaxes:             356\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tifttt (17)\n",
        "\t\thome (15)\n",
        "\t\tlights (15)\n",
        "\t\tautomation (12)\n",
        "\t\tlight (12)\n",
        "\t\trule (12)\n",
        "\t\thouse (10)\n",
        "\t\tturn (10)\n",
        "\t\twemo (8)\n",
        "\t\tswitch (7)\n",
        "\n",
        "Four short links: 20 August 2013\n",
        "\tNum Sentences:           12\n",
        "\tNum Words:               228\n",
        "\tNum Unique Words:        155\n",
        "\tNum Hapaxes:             125\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tit\u2019s (4)\n",
        "\t\ta.i (2)\n",
        "\t\tbbc (2)\n",
        "\t\tbig (2)\n",
        "\t\tcontemporary (2)\n",
        "\t\tfield (2)\n",
        "\t\tintelligence (2)\n",
        "\t\tlooking (2)\n",
        "\t\tnext (2)\n",
        "\t\ttutorials (2)\n",
        "\n",
        "Data Science for Business\n",
        "\tNum Sentences:           16\n",
        "\tNum Words:               344\n",
        "\tNum Unique Words:        164\n",
        "\tNum Hapaxes:             109\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tdata (16)\n",
        "\t\tbusiness (9)\n",
        "\t\tscience (7)\n",
        "\t\tneed (4)\n",
        "\t\tfoster (3)\n",
        "\t\tit\u2019s (3)\n",
        "\t\tleaders (3)\n",
        "\t\tunderstand (3)\n",
        "\t\tusing (3)\n",
        "\t\twork (3)\n",
        "\n",
        "Four short links: 19 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tNum Sentences:           14\n",
        "\tNum Words:               274\n",
        "\tNum Unique Words:        179\n",
        "\tNum Hapaxes:             141\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tidentity (3)\n",
        "\t\tsound (3)\n",
        "\t\tdifferent (2)\n",
        "\t\tflowers (2)\n",
        "\t\the\u2019s (2)\n",
        "\t\tinternet (2)\n",
        "\t\ti\u2019ve (2)\n",
        "\t\tparticular (2)\n",
        "\t\treally (2)\n",
        "\t\twould (2)\n",
        "\n",
        "Four short links: 16 August 2013\n",
        "\tNum Sentences:           11\n",
        "\tNum Words:               205\n",
        "\tNum Unique Words:        132\n",
        "\tNum Hapaxes:             99\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\trevenue (4)\n",
        "\t\tstore (4)\n",
        "\t\tapp (3)\n",
        "\t\tgoogle (3)\n",
        "\t\tmuch (3)\n",
        "\t\tandroid (2)\n",
        "\t\tapps (2)\n",
        "\t\texpressions (2)\n",
        "\t\tplay (2)\n",
        "\t\tregular (2)\n",
        "\n",
        "The vanishing cost of guessing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tNum Sentences:           32\n",
        "\tNum Words:               692\n",
        "\tNum Unique Words:        335\n",
        "\tNum Hapaxes:             246\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tdata (13)\n",
        "\t\tit\u2019s (8)\n",
        "\t\tbig (7)\n",
        "\t\tcream (6)\n",
        "\t\tice (6)\n",
        "\t\tdrowning (4)\n",
        "\t\tmakes (4)\n",
        "\t\tcorrelation (3)\n",
        "\t\tfind (3)\n",
        "\t\tfuture (3)\n",
        "\n",
        "Four short links: 15 August 2013\n",
        "\tNum Sentences:           10\n",
        "\tNum Words:               218\n",
        "\tNum Unique Words:        144\n",
        "\tNum Hapaxes:             113\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\twscs (3)\n",
        "\t\tactivity (2)\n",
        "\t\tcomputer (2)\n",
        "\t\tdatacenter (2)\n",
        "\t\tday (2)\n",
        "\t\tdownloads (2)\n",
        "\t\tgithub (2)\n",
        "\t\tmassive (2)\n",
        "\t\tone (2)\n",
        "\t\trelease (2)\n",
        "\n",
        "Four short links: 14 August 2013\n",
        "\tNum Sentences:           5\n",
        "\tNum Words:               145\n",
        "\tNum Unique Words:        106\n",
        "\tNum Hapaxes:             87\n",
        "\tTop 10 Most Frequent Words (sans stop words):\n",
        "\t\tstories (3)\n",
        "\t\tcompany (2)\n",
        "\t\tfast (2)\n",
        "\t\tfeature (2)\n",
        "\t\tlabs (2)\n",
        "\t\t2013 (1)\n",
        "\t\tadvantage (1)\n",
        "\t\talgorithm (1)\n",
        "\t\tarticle (1)\n",
        "\t\tarticles (1)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_summaries()\n",
      "save_html_summaries()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Four short links: 28 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "Juju \u2014 Canonical\u2019s cloud orchestration software, intended to be a peer of chef and puppet. (via svrn ) \n",
        " Cultural Heritage Symbols \u2014 workshopped icons to indicate interactives, big data, makerspaces, etc. (via Courtney Johnston ) \n",
        " Quinn Norton: Students as Hackers (EdTalks) \u2014 if you really want to understand the future, don\u2019t look at how people are looking at technology, look at how they are misusing technology . noflo.js \u2014 visual flow controls for Javascript.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "(via svrn ) \n",
        " Cultural Heritage Symbols \u2014 workshopped icons to indicate interactives, big data, makerspaces, etc.\n",
        "\n",
        "Four short links: 27 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "Bomb in the Garden (Matthew Butterick) \u2014 de\u00adsign ex\u00adcel\u00adlence is in\u00adhib\u00adit\u00aded by two struc\u00adtur\u00adal flaws in the web. First flaw: the web is good at mak\u00ading in\u00adfor\u00adma\u00adtion free, but ter\u00adri\u00adble at mak\u00ading it ex\u00adpen\u00adsive. Second flaw: the process of adopt\u00ading and en\u00adforc\u00ading web stan\u00addards, as led by the W3C, is hope\u00adless\u00adly bro\u00adken. (via Alex Dong ) \n",
        " Google\u2019s New Play Store Policies on Ads (The Next Web) \u2014 the walls of civilisation holding back the hordes of assclowns. (And gosh, how those \u201cbut I used numbers and symbols!\u201d passwords fall) \n",
        " Brackets (Github) \u2014 open source web code editor by Adobe.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "(via Alex Dong ) \n",
        " Google\u2019s New Play Store Policies on Ads (The Next Web) \u2014 the walls of civilisation holding back the hordes of assclowns. (And gosh, how those \u201cbut I used numbers and symbols!\u201d passwords fall) \n",
        " Brackets (Github) \u2014 open source web code editor by Adobe.\n",
        "\n",
        "Four short links: 26 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "Peruvian Archaeologists Use Drones (Guardian) \u2014 Small drones have been helping a growing number of researchers produce three-dimensional models of Peruvian sites instead of the usual flat maps \u2013 and in days and weeks instead of months and years. Drone Crashes Into Crowd at Great Bull Run (WTVR) \u2014 just what it says. (via DIY Drones ) \n",
        " jsPDF \u2014 create PDF in Javascript on the client. Let\u2019s Make Robots: BoB \u2014 instructions on building a bipedal robot. (via Makezine )\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "Peruvian Archaeologists Use Drones (Guardian) \u2014 Small drones have been helping a growing number of researchers produce three-dimensional models of Peruvian sites instead of the usual flat maps \u2013 and in days and weeks instead of months and years.\n",
        "\n",
        "Four short links: 23 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "Bradley Manning and the Two Americas (Quinn Norton) \u2014 The first America built the Internet, but the second America moved onto it. The best explanation you\u2019ll find for wtf is going on. Staggering Cost of Inventing New Drugs (Forbes) \u2014 $5BB to develop a new drug; and subject to an inverse-Moore\u2019s law: A 2012 article in Nature Reviews Drug Discovery says the number of drugs invented per billion dollars of R&D invested has been cut in half every nine years for half a century. Who\u2019s Watching You \u2014 (Tim Bray) threat modelling. Data Mining with Weka \u2014 learn data mining with the popular open source Weka platform.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "Staggering Cost of Inventing New Drugs (Forbes) \u2014 $5BB to develop a new drug; and subject to an inverse-Moore\u2019s law: A 2012 article in Nature Reviews Drug Discovery says the number of drugs invented per billion dollars of R&D invested has been cut in half every nine years for half a century.\n",
        "\n",
        "Shakespeare and the myth of publishing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        ", which discussed the editor for the First Folio edition of Shakespeare\u2019s plays. We have a mess of good quarto editions, bad quartos , the First Folio, apocryphal works, and more. One important (and undervalued) aspect of this change is that we no longer need the publishing myth of the static text. I\u2019ve argued for a few years now that the book isn\u2019t really what\u2019s important, it\u2019s the conversation about the book. For a technical book, that discussion would contain questions, answers, and clarifications.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        ", which discussed the editor for the First Folio edition of Shakespeare\u2019s plays. The process of editing creates the impression, the mythology, that a carefully crafted, consistent, and stable text exists for these plays, that the plays are static literary objects. We have a mess of good quarto editions, bad quartos , the First Folio, apocryphal works, and more. Perhaps the worst case is Christopher Marlowe\u2019s Doctor Faustus , which is known entirely through two early print editions, one roughly 50% longer than the other. The published versions of the plays were assembled without any involvement from Shakespeare \u2014 indeed, the First Folio was published seven years after his death. And it\u2019s generally assumed that the bad quartos had to do with actors with poor memories, or who perhaps were paid in too much beer. Plays aren\u2019t static, fixed literary objects; they change, according to the demands of the situation. It\u2019s a device that needs a stable, settled text. One important (and undervalued) aspect of this change is that we no longer need the publishing myth of the static text. At O\u2019Reilly, we publish print books and ebooks. The text is no longer static and canonical: it\u2019s living and changing. Collaboration is equally critical to modern technical authorship. But it\u2019s difficult with our older toolsets: emailing Word files back and forth is painful, and while SVN is pretty good at merging changes on line-oriented source code, prose isn\u2019t really line-oriented. I\u2019ve argued for a few years now that the book isn\u2019t really what\u2019s important, it\u2019s the conversation about the book. For a technical book, that discussion would contain questions, answers, and clarifications. The big question is: what can we do now that we\u2019re no longer tied to the myth of stable literary objects?\n",
        "\n",
        "Four short links: 22 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "bletchley (Google Code) \u2014 Bletchley is currently in the early stages of development and consists of tools which provide: Automated token encoding detection (36 encoding variants); Passive ciphertext block length and repetition analysis; Script generator for efficient automation of HTTP requests; A flexible, multithreaded padding oracle attack library with CBC-R support. Hackers of the Renaissance \u2014 Four centuries ago, information was as tightly guarded by intellectuals and their wealthy patrons as it is today. But a few episodes around 1600 confirm that the Hacker Ethic and its attendant emphasis on open-source information and a \u201chands-on imperative\u201d was around long before computers hit the scene. (via BoingBoing ) \n",
        " Maker Camp 2013: A Look Back (YouTube) \u2014 This summer, over 1 million campers made 30 cool projects, took 6 epic field trips, and met a bunch of awesome makers. huxley (Github) \u2014 Watches you browse, takes screenshots, tells you when they change.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "bletchley (Google Code) \u2014 Bletchley is currently in the early stages of development and consists of tools which provide: Automated token encoding detection (36 encoding variants); Passive ciphertext block length and repetition analysis; Script generator for efficient automation of HTTP requests; A flexible, multithreaded padding oracle attack library with CBC-R support. (via BoingBoing ) \n",
        " Maker Camp 2013: A Look Back (YouTube) \u2014 This summer, over 1 million campers made 30 cool projects, took 6 epic field trips, and met a bunch of awesome makers.\n",
        "\n",
        "Four short links: 21 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "blinkdb \u2014 The current version of BlinkDB supports a slightly constrained set of SQL-style declarative queries and provides approximate results for standard SQL aggregate queries, specifically queries involving COUNT, AVG, SUM and PERCENTILE and is being extended to support any User-Defined Functions (UDFs). sheetsee.js (github) \u2014 Javascript library that makes it easy to use a Google Spreadsheet as the database feeding the tables, charts and maps on a website. (via Tom Armitage ) \n",
        " China Plans to Become a Leader in Robotics (Quartz) \u2014 The ODCCC too funds high risk research initiatives through the Thousand Talent Project (TTP), a three-year term project with possible extension. The goal of the TTP is to recruit thousands of foreign researchers with strong expertise in hardware and software to help develop innovation in China. AppScale (GitHub) \u2014 open source implementation of Google App Engine.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "blinkdb \u2014 The current version of BlinkDB supports a slightly constrained set of SQL-style declarative queries and provides approximate results for standard SQL aggregate queries, specifically queries involving COUNT, AVG, SUM and PERCENTILE and is being extended to support any User-Defined Functions (UDFs). (via Tom Armitage ) \n",
        " China Plans to Become a Leader in Robotics (Quartz) \u2014 The ODCCC too funds high risk research initiatives through the Thousand Talent Project (TTP), a three-year term project with possible extension.\n",
        "\n",
        "If This/Then That (IFTTT) and the Belkin WeMo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---------------------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "As you\u2019ll see, IFTTT lets you choose a location and use information from Internet services to assemble the context. ) \n",
        " \n",
        " To act in the physical realm of my house, though, IFTTT requires devices that can take action. However, the weather channel also allows you to take actions when conditions change. Sunset is straightforward because it\u2019s known for the location that you chose: \n",
        "  \n",
        " Here at step three, the first part of the rule is done. The combination of IFTTT and the WeMo makes setup much easier and more accessible.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "The only disadvantage to the toggle switch is that you have to touch it to operate it, and that means getting off the couch. At that point, remote controlled lights seemed pretty darn attractive. My first lighting automation was done with Insteon , a superset of the venerable 1970s-era X10 protocol . Earlier this year, I was introduced to IFTTT , which stands for \u201cIf This, Then That.\u201d\u00a0 It is the closest thing I have seen to a generic software stack that can readily be used for home automation purposes. There is much to like about IFTTT, so I\u2019ll focus on just a few attributes: \n",
        " \n",
        " Programming without knowing programming. Traditional home automation has been based on sensors inside the home. To turn the lights on after dark, you need a sensor that tells your house it\u2019s dark. As you\u2019ll see, IFTTT lets you choose a location and use information from Internet services to assemble the context. ) \n",
        " \n",
        " To act in the physical realm of my house, though, IFTTT requires devices that can take action. The Belkin WeMo is a family of products, including a remote-controlled electrical outlet, light switch, and motion sensor. So, let\u2019s start out with a simple idea: I have a light, and I\u2019d like to turn the light on when it\u2019s dark. In traditional home automation, I can easily do that by picking a time to turn the light on and using the same time every day. If I pick a time that is appropriate for the winter, though, the light will come on too early in June. If I pick a time that is right for June, I\u2019ll have to get up and turn the lights on in December when it gets dark. However, the weather channel also allows you to take actions when conditions change. Sunset is straightforward because it\u2019s known for the location that you chose: \n",
        "  \n",
        " Here at step three, the first part of the rule is done. We have set up a rule that will fire every day at sunset. In our case, we want to turn on the outlet to turn on the light: \n",
        "  \n",
        " From the menu of options, choose \u201cturn on.\u201d\u00a0 IFTTT supports having multiple WeMo switches, each of which can be named. A drop-down allows you to choose the switch being controlled by the rule so that many different devices can be controlled. There is not yet an easy way to say \u201cturn off the light two hours after it turned on\u201d because IFTTT doesn\u2019t hold much state (yet). ) \n",
        "  \n",
        " Could I have accomplished the same tasks in other home automation systems? The combination of IFTTT and the WeMo makes setup much easier and more accessible.\n",
        "\n",
        "Four short links: 20 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "BBC Forum \u2014 American social psychologist Aleks Krotoski has been looking at how the internet affects the way we talk to ourselves. (via Vaughan Bell ) \n",
        " Why Can\u2019t My Computer Understand Me (New Yorker) \u2014 using anaphora as the basis of an intelligence test, as example of what AI should be striving for. In Levesque\u2019s view, the field of artificial intelligence has fallen into a trap of \u201cserial silver bulletism,\u201d always looking to the next big thing, whether it\u2019s expert systems or Big Data, but never painstakingly analyzing all of the subtle and deep knowledge that ordinary human beings possess. That\u2019s a gargantuan task\u2014 \u201cmore like scaling a mountain than shoveling a driveway,\u201d as Levesque writes. 507 Mechanical Movements \u2014 an old basic engineering textbook, animated .\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "BBC Forum \u2014 American social psychologist Aleks Krotoski has been looking at how the internet affects the way we talk to ourselves. In Levesque\u2019s view, the field of artificial intelligence has fallen into a trap of \u201cserial silver bulletism,\u201d always looking to the next big thing, whether it\u2019s expert systems or Big Data, but never painstakingly analyzing all of the subtle and deep knowledge that ordinary human beings possess. 507 Mechanical Movements \u2014 an old basic engineering textbook, animated .\n",
        "\n",
        "Data Science for Business\n",
        "-------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "Foster and Tom have a long history of applying data to practical business problems. It wasn\u2019t about coding: business students don\u2019t need to learn how to implement machine learning algorithms in Python. It is about business: specifically, it\u2019s about the data analytic thinking that business people need to work with data effectively. But in this data-driven age, it\u2019s critically important for business leaders to understand how to work with the data scientists on their teams. In today\u2019s business world, it\u2019s essential to understand which algorithms are used for different applications, how statistics are used to create models of human and economic behavior, overfitting and its symptoms, and much more.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "Foster and Tom have a long history of applying data to practical business problems. It wasn\u2019t about coding: business students don\u2019t need to learn how to implement machine learning algorithms in Python. But in this data-driven age, it\u2019s critically important for business leaders to understand how to work with the data scientists on their teams. In today\u2019s business world, it\u2019s essential to understand which algorithms are used for different applications, how statistics are used to create models of human and economic behavior, overfitting and its symptoms, and much more.\n",
        "\n",
        "Four short links: 19 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "choir.io explained (Alex Dong) \u2014 Sound is the perfect medium for wearable computers to talk back to us. Identity Single Point of Failure (Tim Bray) \u2014 continuing his excellent series on federated identity. (The Atlantic) \u2014 Rather, the failures will come in the form of consumers being systematically charged more than they would have been had less information about that particular consumer. Sometimes, that will mean exploiting people who are not of a particular class, say upcharging men for flowers if a computer recognizes that that he\u2019s looking for flowers the day after his anniversary. (via Slashdot ) \n",
        " Life Inside Brewster\u2019s Magnificent Contraption (Jason Scott) \u2014 I\u2019ve been really busy.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "choir.io explained (Alex Dong) \u2014 Sound is the perfect medium for wearable computers to talk back to us. Identity Single Point of Failure (Tim Bray) \u2014 continuing his excellent series on federated identity. (The Atlantic) \u2014 Rather, the failures will come in the form of consumers being systematically charged more than they would have been had less information about that particular consumer. (via Slashdot ) \n",
        " Life Inside Brewster\u2019s Magnificent Contraption (Jason Scott) \u2014 I\u2019ve been really busy.\n",
        "\n",
        "Four short links: 16 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "The primary goal of this library is to generate regular expressions from a known set of inputs which avoid backtracking as much as possible. (via Pete Warden ) \n",
        " Gumbo \u2014 open source pure C HTML5 parser. By contrast, Apple\u2019s App Store revenue grew 15%, according to Distimo estimates, which cover the top 18 countries. That sounds less impressive if you consider that the App Store brought in twice as much revenue in absolute terms. In February, the App Store was generating three times as much revenue, and last November it out-earned Google Play by a factor of four.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "(via Pete Warden ) \n",
        " Gumbo \u2014 open source pure C HTML5 parser. By contrast, Apple\u2019s App Store revenue grew 15%, according to Distimo estimates, which cover the top 18 countries. In February, the App Store was generating three times as much revenue, and last November it out-earned Google Play by a factor of four.\n",
        "\n",
        "The vanishing cost of guessing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:8: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
        "Content saved in: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 28 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 28 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 27 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 27 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 26 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 26 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 23 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 23 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Shakespeare and the myth of publishing.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Shakespeare and the myth of publishing.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 22 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 22 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 21 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 21 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/If This-Then That (IFTTT) and the Belkin WeMo.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/If This-Then That (IFTTT) and the Belkin WeMo.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 20 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 20 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Data Science for Business.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Data Science for Business.summary.mean_scored_summary.html\n",
        "Content saved in: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 19 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 19 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 16 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 16 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "These kinds of correlations are all around us, and big data makes them easy to find. But a quantified society, awash in data, might be tempted to do so because overwhelming correlation looks a lot like causality. And overwhelming correlation is what big data does best. On the other hand, critics charge that big data will make us stick to constantly optimizing what we already know, rather than thinking out of the box and truly innovating. As we become an increasingly data-driven society, it\u2019s critical that we remember we can no more predict tomorrow with today\u2019s data than we can prevent drowning by banning ice cream.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "These kinds of correlations are all around us, and big data makes them easy to find. But a quantified society, awash in data, might be tempted to do so because overwhelming correlation looks a lot like causality. And overwhelming correlation is what big data does best. On the other hand, critics charge that big data will make us stick to constantly optimizing what we already know, rather than thinking out of the box and truly innovating. As we become an increasingly data-driven society, it\u2019s critical that we remember we can no more predict tomorrow with today\u2019s data than we can prevent drowning by banning ice cream.\n",
        "\n",
        "Four short links: 15 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "Makies Hit Shelves at Selfridges \u2014 3d printing business gaining mainstream distribution. We describe the architecture of WSCs, the main factors influencing their design, operation, and cost structure, and the characteristics of their software base. (via Mike Loukides ) \n",
        " Illegal Downloads Not Erased By Simultaneous Release \u2014 Data gathered by TorrentFreak throughout the day reveals that most early downloaders, a massive 16.1%, come from Australia. Down Under the show aired on the pay TV network Foxtel, but it appears that many Aussies prefer to download a copy instead. The same is true for the United States and Canada, with 16% and 9.6% of the total downloads respectively, despite the legal offerings.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "Makies Hit Shelves at Selfridges \u2014 3d printing business gaining mainstream distribution. (via Mike Loukides ) \n",
        " Illegal Downloads Not Erased By Simultaneous Release \u2014 Data gathered by TorrentFreak throughout the day reveals that most early downloaders, a massive 16.1%, come from Australia. Down Under the show aired on the pay TV network Foxtel, but it appears that many Aussies prefer to download a copy instead.\n",
        "\n",
        "Four short links: 14 August 2013\n",
        "--------------------------------\n",
        "\n",
        "-------------\n",
        "Top N Summary\n",
        "-------------\n",
        "bookcision \u2014 bookmarklet to download your Kindle highlights. (via Nelson Minar ) \n",
        " Algorithm for a Perfectly Balanced Photo Gallery \u2014 remember this when it comes time to lay out your 2013 \u201cHappy Holidays!\u201d card. Long Stories (Fast Company Labs) \u2014 Our strategy was to still produce feature stories as discrete articles, but then to tie them back to the stub article with lots of prominent links, again taking advantage of the storyline and context we had built up there, making our feature stories sharper and less full of catch-up material. Massachusetts Software Tax (Fast Company Labs) \u2014 breakdown of why this crappily-written law is bad news for online companies. Laws are the IEDs of the Internet: it\u2019s easy to make massively value-destroying regulation and hard to get it fixed.\n",
        "\n",
        "-------------------\n",
        "Mean Scored Summary\n",
        "-------------------\n",
        "(via Nelson Minar ) \n",
        " Algorithm for a Perfectly Balanced Photo Gallery \u2014 remember this when it comes time to lay out your 2013 \u201cHappy Holidays!\u201d card.\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/The vanishing cost of guessing.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/The vanishing cost of guessing.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 15 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 15 August 2013.summary.mean_scored_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 14 August 2013.summary.top_n_summary.html\n",
        "Content saved in:  /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 14 August 2013.summary.mean_scored_summary.html\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extract_entities()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Four short links: 28 August 2013\n",
        "--------------------------------\n",
        "\tJuju (1)\n",
        "\tCultural Heritage Symbols \u2014 (1)\n",
        "\tCourtney Johnston ) Quinn Norton (1)\n",
        "\tStudents (1)\n",
        "\n",
        "Four short links: 27 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tW3C (1)\n",
        "\tBomb (1)\n",
        "\tFirst (1)\n",
        "\tInside Password Cracking ( Wired ) \u2014 (1)\n",
        "\tAds ( The Next Web ) \u2014 (1)\n",
        "\tBrackets (1)\n",
        "\tImagine (1)\n",
        "\tGithub ) \u2014 (1)\n",
        "\tPolicies (1)\n",
        "\n",
        "Four short links: 26 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tUse Drones ( Guardian ) \u2014 Small (1)\n",
        "\tPeruvian (1)\n",
        "\tArchaeologists (1)\n",
        "\tDrone Crashes Into Crowd (1)\n",
        "\tRobots (1)\n",
        "\tJavascript (1)\n",
        "\n",
        "Four short links: 23 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tTim Bray ) (1)\n",
        "\tInventing New Drugs ( Forbes ) \u2014 (1)\n",
        "\tCost (1)\n",
        "\tInternet (1)\n",
        "\tEveryone (1)\n",
        "\tNature Reviews Drug Discovery (1)\n",
        "\tBradley Manning (1)\n",
        "\tR (1)\n",
        "\tWeka (1)\n",
        "\tAmerica (2)\n",
        "\tData Mining (1)\n",
        "\tAmericas ( Quinn Norton ) \u2014 The (1)\n",
        "\tWeka \u2014 (1)\n",
        "\tD (1)\n",
        "\n",
        "Shakespeare and the myth of publishing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------------\n",
        "\tScene (2)\n",
        "\tCondell (1)\n",
        "\tShakespeare (4)\n",
        "\tShakespeare \u2014 (1)\n",
        "\tAct (1)\n",
        "\tBen Jonson (1)\n",
        "\tTim (1)\n",
        "\tPlays (1)\n",
        "\tApocrypha\u201d (1)\n",
        "\tQuarto (1)\n",
        "\tFirst Folio (2)\n",
        "\tWord (1)\n",
        "\tCan (2)\n",
        "\tShakespeare (2)\n",
        "\tShakespeare (1)\n",
        "\tChapter (1)\n",
        "\tCollaboration (1)\n",
        "\tSound (1)\n",
        "\tFirst Folio (2)\n",
        "\tFolio (1)\n",
        "\tHeminges (1)\n",
        "\tScripts (1)\n",
        "\tV (1)\n",
        "\tKing Lear (1)\n",
        "\tFoo Camp (1)\n",
        "\tFaulkner (1)\n",
        "\tFury (1)\n",
        "\tWho Edited Shakespeare (1)\n",
        "\n",
        "Four short links: 22 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tWeb (1)\n",
        "\tScript (1)\n",
        "\tRenaissance \u2014 Four (1)\n",
        "\tHuxley (1)\n",
        "\tGithub ) \u2014 Watches (1)\n",
        "\tGoogle Code ) \u2014 Bletchley (1)\n",
        "\tHacker Ethic (1)\n",
        "\tPassive (1)\n",
        "\tHackers (1)\n",
        "\n",
        "Four short links: 21 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tLeader (1)\n",
        "\tTom Armitage ) China Plans (1)\n",
        "\tGoogle Spreadsheet (1)\n",
        "\tBecome (1)\n",
        "\tGoogle (1)\n",
        "\tChina (1)\n",
        "\tQueries (1)\n",
        "\tFunctions (1)\n",
        "\tChina (1)\n",
        "\tJavascript (1)\n",
        "\n",
        "If This/Then That (IFTTT) and the Belkin WeMo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---------------------------------------------\n",
        "\tInternet (1)\n",
        "\tInsert (1)\n",
        "\tThen That.\u201d It (1)\n",
        "\tF (1)\n",
        "\tJune (1)\n",
        "\tInsteon (1)\n",
        "\tInternet (1)\n",
        "\tAsterisk (1)\n",
        "\tWi-Fi (1)\n",
        "\tTriggers (1)\n",
        "\tW (1)\n",
        "\tSunset (1)\n",
        "\tSan Francisco (1)\n",
        "\tMister House (1)\n",
        "\tX10 (1)\n",
        "\tDecember (1)\n",
        "\tInsteon (2)\n",
        "\tJune (1)\n",
        "\tWeather (1)\n",
        "\t\u201cW\u201d (1)\n",
        "\n",
        "Four short links: 20 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tLevesque (1)\n",
        "\tMechanical Movements \u2014 (1)\n",
        "\tKrotoski (1)\n",
        "\tAleks (1)\n",
        "\tGoogle (1)\n",
        "\tBig Data (1)\n",
        "\tA.I (1)\n",
        "\tA.I (1)\n",
        "\tPodcast (1)\n",
        "\n",
        "Data Science for Business"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "-------------------------\n",
        "\tFoster (1)\n",
        "\tHadoop (1)\n",
        "\tData Science (2)\n",
        "\tR (1)\n",
        "\tClaudia Perlich (1)\n",
        "\tPython (1)\n",
        "\tBusiness (1)\n",
        "\tBusiness (2)\n",
        "\tTom (1)\n",
        "\tFoster Provost (1)\n",
        "\tTom Fawcett (1)\n",
        "\tFoster (1)\n",
        "\tData (1)\n",
        "\n",
        "Four short links: 19 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tAlex Dong ) \u2014 Sound (1)\n",
        "\tAtlantic ) \u2014 Rather (1)\n",
        "\tFailure ( Tim Bray ) \u2014 (1)\n",
        "\tSound (1)\n",
        "\tIdentity (1)\n",
        "\tInternet Archive (1)\n",
        "\tGoogle (1)\n",
        "\tDifferent (1)\n",
        "\tIdentity (1)\n",
        "\tSingle Point (1)\n",
        "\tInternet (1)\n",
        "\tMatter If Companies Are Tracking Us Online (1)\n",
        "\tEric Sachs (1)\n",
        "\n",
        "Four short links: 16 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tAndroid (1)\n",
        "\tApp Store (1)\n",
        "\tPeople With Cheap Android Phones Have Started Buying Apps ( Quartz ) \u2014 (1)\n",
        "\tNovember (1)\n",
        "\tApp Store (1)\n",
        "\tGoogle Play (1)\n",
        "\tPete Warden ) Gumbo \u2014 (1)\n",
        "\tDistimo (1)\n",
        "\tNone (1)\n",
        "\tFebruary (1)\n",
        "\tBoolean Trap \u2014 (1)\n",
        "\tGoogle Play Store (1)\n",
        "\n",
        "The vanishing cost of guessing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "------------------------------\n",
        "\tRedlining (1)\n",
        "\tProponents (1)\n",
        "\tConsider (1)\n",
        "\tToday (1)\n",
        "\tLaw (1)\n",
        "\tParallel (1)\n",
        "\tBig (3)\n",
        "\tGoogle (1)\n",
        "\tApartheid (1)\n",
        "\tWatson (1)\n",
        "\n",
        "Four short links: 15 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tAussies (1)\n",
        "\tComputer \u2014 (1)\n",
        "\tWin (1)\n",
        "\tFoxtel (1)\n",
        "\tAustralia (1)\n",
        "\tMakies (1)\n",
        "\tUnited (1)\n",
        "\tUnclear (1)\n",
        "\tMike Loukides ) Illegal Downloads Not Erased By Simultaneous Release \u2014 Data (1)\n",
        "\tShelves (1)\n",
        "\tStates (1)\n",
        "\tCanada (1)\n",
        "\tSelfridges (1)\n",
        "\tDown Under (1)\n",
        "\tDatacenter (1)\n",
        "\n",
        "Four short links: 14 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "\tMassachusetts (1)\n",
        "\tNelson Minar ) Algorithm (1)\n",
        "\tLaws (1)\n",
        "\tStories (1)\n",
        "\tHolidays (1)\n",
        "\tPerfectly Balanced Photo Gallery \u2014 (1)\n",
        "\tTax ( Fast Company Labs ) \u2014 (1)\n",
        "\tFast Company Labs ) \u2014 Our (1)\n",
        "\tInternet (1)\n",
        "\tKindle (1)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_interactions()\n",
      "save_html_interactions()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Four short links: 28 August 2013\n",
        "--------------------------------\n",
        "Juju; cloud orchestration software; peer; chef\n",
        "(; svrn; Cultural Heritage Symbols \u2014; icons; interactives; data; makerspaces\n",
        "(; Courtney Johnston ) Quinn Norton; Students; Hackers ( EdTalks ) \u2014; future; look; people; technology\n",
        "noflo.js; flow; controls\n",
        "\n",
        "Four short links: 27 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "Bomb; Garden ( Matthew Butterick ) \u2014 de\u00adsign ex\u00adcel\u00adlence; flaws\n",
        "First; web\n",
        "web; un\u00adder\n",
        "flaw; process; web; stan\u00addards\n",
        "(; Alex Dong ) Google\u2019s New Play Store; Policies; Ads ( The Next Web ) \u2014; walls; civilisation; hordes\n",
        "Imagine; behaviour\n",
        "Inside Password Cracking ( Wired ) \u2014; pros; password\n",
        "(; numbers; symbols; passwords; Brackets; Github ) \u2014; source web code editor\n",
        "\n",
        "Four short links: 26 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "Archaeologists; Use Drones ( Guardian ) \u2014 Small; number; researchers; models; Peruvian; maps; days; weeks; months\n",
        "\n",
        "(; DIY Drones ) jsPDF \u2014; create; PDF; Javascript\n",
        "Let\u2019s; Robots; BoB; instructions; building\n",
        "\n",
        "\n",
        "Four short links: 23 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "Bradley Manning; Americas ( Quinn Norton ) \u2014 The; America; built; Internet\n",
        "\n",
        "\n",
        "Cost; Inventing New Drugs ( Forbes ) \u2014; drug; law; article; Nature Reviews Drug Discovery; number; drugs; billion; dollars; R; D; years\n",
        "Who\u2019s; Tim Bray )\n",
        "\n",
        "Data Mining; Weka \u2014; learn; data; source; Weka\n",
        "\n",
        "Shakespeare and the myth of publishing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------------\n",
        "post; Foo Camp\n",
        "weeks; Tim; link\n",
        "editor; First Folio; edition\n",
        "lot; evidence; someone; lot; work; spelling; tasks; copyeditor; proofreader; work; Folio\u2019s; editors; Heminges\n",
        "It\u2019s; argument; thoughts; nature\n",
        "process; editing; impression; mythology; text; exists; plays; plays\n",
        "Shakespeare; text; Shakespeare\n",
        "mess; quarto; editions; quartos; First Folio\n",
        "versions; plays; others; scholars; missing; parts; Macbeth ( Shakespeare\u2019s; shortest tragedy; First Folio\n",
        "case; Christopher Marlowe\u2019s Doctor Faustus; print; editions\n",
        "I\u2019m; search; version\n",
        "Shakespeare\u2019s; plays; performances\n",
        "involvement; theater; III; Scene; iv; cut\n",
        "Act; V; Scene; fill; plays\n",
        "years; Shakespeare; editors; lot; I\u2019m; sure; Shakespeare; theater; partner; theater company\n",
        "versions; plays; involvement; Shakespeare \u2014; First Folio; years\n",
        "Scripts; trade; secrets; theater; companies; playwright; Ben Jonson; anything; publishers\n",
        "publishers; bunch; actors; table; remember\n",
        "quartos; actors; memories\n",
        "It\u2019s; quartos; scripts; evidence\n",
        "It\u2019s; difference; editions; actors; inebriation; performances\n",
        "Plays; objects; demands\n",
        "differences; Quarto; Folio; editions; performances; quality\n",
        "notion; text; Shakespeare; play; myth; institution; publishing; Shakespeare didn\u2019t; part\n",
        "printing press; century press; offset press\n",
        "It\u2019s; device\n",
        "text; myth; text; part\n",
        "\n",
        "publishing industry; change; players\n",
        "(; aspect; change; longer; publishing myth\n",
        "\n",
        "time; ebooks; field; day; book; author; description; Chapter; add; couple; pages\n",
        "text; longer\n",
        "customers; printing\n",
        "author; book; files; POD; printer; customers; order; print book tomorrow; tomorrow\u2019s\n",
        "process; publishing; texts; texts; impart wisdom; author\n",
        "re-invention; survival; survival\n",
        "\n",
        "\n",
        "Shakespeare; plays; entirety; it\u2019s; process\n",
        "master author; plot; scenes; apprentices; members; theater company staff fill\n",
        "There\u2019s; evidence; plays; addition; plays; authorship; Apocrypha\u201d; products\n",
        "\n",
        "difficult; toolsets; Word; files; SVN; merging; changes; source code\n",
        "approach; publishing; collaboration problem; publishing toolchain; collaboration; default\n",
        "years; book isn\u2019t; conversation\n",
        "Shakespeare; Faulkner; books; conversation; King Lear; Sound\n",
        "\n",
        "Can; book; something; communication\n",
        "book; discussion; questions; answers\n",
        "changes; author; response\n",
        "(; work; author; he\u2019s\n",
        "); distinction; draft; access\u201d book; edition; book something\n",
        "happens; notion; collaboration; authors; co-authors\n",
        "Can; process; way\n",
        "reinvention; questions; publishing enterprise; kinds; ebooks; models; compensation; models\n",
        "problems; (; working\n",
        "question; longer; myth\n",
        "\n",
        "\n",
        "Four short links: 22 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "bletchley; Google Code ) \u2014 Bletchley; stages; development; consists; tools; token; detection; variants; Passive; ciphertext block length; repetition analysis; Script; generator; efficient automation; HTTP; requests; oracle attack library\n",
        "Hackers; Renaissance \u2014 Four; centuries; information; intellectuals; patrons\n",
        "episodes; confirm; Hacker Ethic; emphasis; information; imperative\u201d; computers\n",
        "(; BoingBoing ) Maker Camp; Look Back ( YouTube ) \u2014 This; summer; campers; cool; projects; field; trips; bunch; awesome\n",
        "huxley; Github ) \u2014 Watches; screenshots\n",
        "Huxley; system; catching; regressions; Web\n",
        "\n",
        "\n",
        "Four short links: 21 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "blinkdb; version; BlinkDB; supports; set; queries; results; SQL; aggregate; queries; COUNT; AVG; SUM; PERCENTILE; Functions\n",
        "Queries; operations; error bound; time constraint; system\n",
        "sheetsee.js; github; Javascript; library; Google Spreadsheet; database; tables; charts\n",
        "changes; spreadsheet; Google; site; visitor\n",
        "(; Tom Armitage ) China Plans; Become; Leader; Robotics ( Quartz ) \u2014 The ODCCC; risk research; Thousand Talent Project ( TTP ); term project\n",
        "goal; TTP; thousands; researchers; expertise; hardware; software; innovation\n",
        "researchers; China; year\n",
        "AppScale; GitHub; source implementation\n",
        "\n",
        "If This/Then That (IFTTT) and the Belkin WeMo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "---------------------------------------------\n",
        "\n",
        "practice; computer; task; intents\n",
        "fits; starts; years; house\n",
        "life; home; friends; watch; house\n",
        "house; switches; TV; place else; orientation\n",
        "post; I\u2019ll; talk; automation; tasks; lights\n",
        "light switch; century; toggle switch\n",
        "\n",
        "disadvantage; toggle switch\n",
        "home; older home\n",
        "plumbing; wall; house; wall; telephone demarc; side; made; modifications; telephone wiring; days\n",
        "space; built-in; lights; floor\n",
        "\u201citch\u201d; home automation project; consistent forgetfulness; lights\n",
        "\n",
        "lighting automation; Insteon; superset; X10\n",
        "Insteon; controls; RF; remote; lights; locations\n",
        "hardware; place\n",
        "RF; remote\n",
        "home automation community doesn\u2019t; software; lets; things\n",
        "Mister House; while; time; Insteon\n",
        "friend; automation code; tester; problems; homes; something; answer\n",
        "year; IFTTT; stands; Then That.\u201d It; thing; software stack; home automation\n",
        "IFTTT; home automation; I\u2019m; use; purpose\n",
        "IFTTT; focus; attributes\n",
        "computer science; courses; program; I\u2019m\n",
        "IFTTT; lets; rules; programs; control; world; bar; learning; syntax; system administration work\n",
        "\n",
        "home automation; sensors\n",
        "order; something; sensor\n",
        "lights; sensor; house\n",
        "something; raining; sensor\n",
        "see; IFTTT; lets; location; information; Internet; services\n",
        "\n",
        "everybody; brags; design; IFTTT; colleague; sales uses\n",
        "(; Insert; joke; competence\n",
        "); realm; house; IFTTT; devices\n",
        "Belkin WeMo; family; products; outlet; switch\n",
        "WeMo; Wi-Fi; home network; device; network; service\n",
        "let\u2019s; start; idea\n",
        "home automation; picking; time; time\n",
        "time; winter; light\n",
        "time; June; I\u2019ll; lights\n",
        "home automation; source; sunset; times; hope; data\n",
        "\n",
        "example; I\u2019ll; show; WeMo; turns\n",
        "IFTTT mini-programs; trigger\n",
        "recipe; straightforward; process; picking; trigger; interaction; world; \u201cchannels; channels\n",
        "step; creating; recipe; trigger\n",
        "screen shot; F; W; Weather\n",
        "time; weather channel; you\u2019ll; location; home; San Francisco; weather channel; number; components\n",
        "\n",
        "weather channel; actions\n",
        "weather; data; IFTTT; count; UV; index\n",
        "purpose; example; we\u2019ll; \u201csunset\u201d; option; it\u2019s; creating; triggers; control; lights; sky; cloudy; air filtration; pollen count; rises; triggers\n",
        "Sunset; known; location; step; part\n",
        "rule; day\n",
        "IFTTT; \u201cthis\u201d part; rule; way; part; rule; action; \u201cthat\u201d; trigger\n",
        "channel; action; number; channels; trying; control; lights; Belkin WeMo; scroll; \u201cW\u201d\n",
        "actions; power switch; turn; outlet; turn; outlet; blink; outlet\n",
        "case; outlet; menu; options; on.\u201d; IFTTT; supports; multiple\n",
        "allows; switch; rule\n",
        "example; coffeepot; alarm; lights; rules; step; creating; rule\n",
        "IFTTT; rule\n",
        "doubt; rule; setup; rule; p.m.\n",
        "way; hours; IFTTT doesn\u2019t; state\n",
        "aspect; IFTTT\n",
        "town; lights; night; pet; sitters; ways\n",
        "off; rule; rule; lights\n",
        "(; lights; question; strand; LED\n",
        "); tasks; home automation\n",
        "\n",
        "combination; IFTTT; WeMo\n",
        "\n",
        "Four short links: 20 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "pineapple.io; attempt; rankings; tutorials; products; way; Google; search; results; tutorials; incompetent; illiterates; versions\n",
        "BBC Forum \u2014 American; psychologist; Aleks; Krotoski; internet; way\n",
        "Podcast; days\n",
        "(; Vaughan Bell ) Why Can\u2019t My Computer Understand Me ( New Yorker ) \u2014; anaphora; basis; intelligence test; example\n",
        "\n",
        "kinds; problems; it\u2019s\n",
        "\n",
        "view; field; intelligence; trap; silver bulletism; thing; it\u2019s; expert; systems; Big Data; subtle; knowledge; human\n",
        "That\u2019s; gargantuan task\u2014; scaling; mountain; driveway; Levesque\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Data Science for Business"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "-------------------------\n",
        "couple; years; Claudia Perlich; Foster Provost; PhD\n",
        "Foster; book; Tom Fawcett; teaching\n",
        "Foster; Tom; history; data; business\n",
        "book; Data Science; Business; data; science; books\n",
        "tools; Hadoop\n",
        "business; students; need; machine; algorithms\n",
        "business; data; business; people\n",
        "Data; thinking; questions; questions; answers\n",
        "Business; leaders; (; ); data\n",
        "age; it\u2019s; business; leaders; data scientists\n",
        "business world; it\u2019s; algorithms; applications; statistics; models; behavior\n",
        "machine learning algorithm; ideas; data scientists\n",
        "goal; data; science\n",
        "That\u2019s; Data Science; Business\n",
        "books; data; science; number; programs; data\n",
        "anything; teaches data; science; leaders; data\n",
        "\n",
        "Four short links: 19 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "choir.io; Alex Dong ) \u2014 Sound; perfect medium\n",
        "Sound; dozen; properties; level; emotions\n",
        "Different; packs\n",
        "Identity; Single Point; Failure ( Tim Bray ) \u2014; excellent series\n",
        "There\u2019s; guy; Google; Eric Sachs; who\u2019s; Identity; stuff; center; Internet; universe; lot\n",
        "mantras; typing; password; something; engineers; security; abuse; fraud\n",
        "\n",
        "(; Atlantic ) \u2014 Rather; failures; form; consumers; information\n",
        "exploiting; people; class; upcharging; men; flowers; computer; flowers\n",
        "summary; Ryan Calo\u2019s\n",
        "\n",
        "upload; statistics; here\u2019s; Internet Archive; objects\n",
        "\n",
        "\n",
        "\n",
        "Four short links: 16 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "frak; transforms collections; strings; expressions\n",
        "goal; library; expressions; known set; inputs\n",
        "Boolean Trap \u2014; crappy\n",
        "\n",
        "(; Pete Warden ) Gumbo \u2014; source pure; C HTML5\n",
        "People With Cheap Android Phones Have Started Buying Apps ( Quartz ) \u2014; revenue; Google Play Store; Android; users; apps\n",
        "contrast; Apple\u2019s App Store; revenue; %; Distimo; estimates\n",
        "App Store; twice; revenue; absolute\n",
        "\n",
        "February; App Store; times; revenue; November; Google Play\n",
        "\n",
        "\n",
        "The vanishing cost of guessing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "------------------------------\n",
        "\n",
        "\n",
        "It\u2019s; ice cream; happen\n",
        "ice cream consumption; predictor; fatalities\n",
        "kinds; correlations\n",
        "childhood trauma; obesity; nutrition; crime; rates; toddlers\n",
        "ban ice cream; hopes; drowning; someone\n",
        "society; awash; data; correlation; looks; lot\n",
        "\n",
        "\n",
        "Parallel; computing; advances; algorithms; crawl; Law\n",
        "Consider; activity; dozens; times; day; Google\n",
        "search; thousands; machines; returns hundreds\n",
        "Big; data\n",
        "Google\u2019s; results; right\n",
        "Watson; IBM\u2019s Jeopardy-winning; software; millions; records; guess\n",
        "Today; abundance; cheap; tools; organizations; guess; everything; employee honesty; spread; disease; delivery; car; parts; city\n",
        "Tomorrow\u2019s; society; smarter\n",
        "implications; shift; point; looks; lot\n",
        "data; revolution\n",
        "choosing; right ad; web visitor; insurance premium; inner-city student learn; reams\n",
        "Proponents; boon\n",
        "Big; data; smart; flu outbreak\n",
        "transparent; information; tools; light; data; corruption; opinions\n",
        "hand; critics; data\n",
        "machines; improvements\n",
        "abundance; data; facts; notions\n",
        "correlation; causality; someone; coverage; employment; pattern; control; racism; injustice; Apartheid\n",
        "Big; data; tool\n",
        "It\u2019s; way; peer; going; it\u2019s\n",
        "cost; guess\n",
        "society; it\u2019s; tomorrow; data\n",
        "\n",
        "Four short links: 15 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "github; activity; audio; github activity\n",
        "Makies; Shelves; Selfridges; business\n",
        "\n",
        "Datacenter; Computer \u2014; datacenter; computer\n",
        "architecture; WSCs; factors; design; operation; structure; characteristics\n",
        "architects; programmers; today\u2019s; WSCs; platforms; day implement; equivalent; WSCs\n",
        "(; Mike Loukides ) Illegal Downloads Not Erased By Simultaneous Release \u2014 Data; TorrentFreak; day; reveals; downloaders; %; come\n",
        "Down Under; show; pay TV network; Foxtel; Aussies\n",
        "United; States; Canada; %; %; downloads\n",
        "Unclear; represents\n",
        "\n",
        "Four short links: 14 August 2013"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 28 August 2013.interactions.html\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 27 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 26 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 23 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Shakespeare and the myth of publishing.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 22 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 21 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/If This-Then That (IFTTT) and the Belkin WeMo.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 20 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Data Science for Business.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 19 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 16 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/The vanishing cost of guessing.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 15 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Content saved in: /home/rsouza/Dropbox/Renato/ModMinDados/outputs/blogs_out/Four short links: 14 August 2013.interactions.html"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------------------------------\n",
        "bookcision; bookmarklet; Kindle\n",
        "(; Nelson Minar ) Algorithm; Perfectly Balanced Photo Gallery \u2014; remember; time; Holidays\n",
        "Stories; Fast Company Labs ) \u2014 Our; strategy; feature; stories; articles; stub article; lots; prominent; links; advantage; storyline; context; feature; stories\n",
        "Massachusetts; Tax ( Fast Company Labs ) \u2014; breakdown; law; news; online\n",
        "Laws; IEDs; Internet\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.pos_tag(['a','sentence','in','english', 'with', 'waldisney'])\n",
      "#nltk.tag.pos_tag(nltk.word_tokenize(\"John's big idea isn't all that bad.\"))\n",
      "#nltk.help.upenn_tagset()\n",
      "\n",
      "#tsents = nltk.corpus.floresta.tagged_sents()\n",
      "tsents = nltk.corpus.mac_morpho.tagged_sents()\n",
      "tsents = [[(w.lower(),nltk.simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]\n",
      "tagger0 = nltk.tag.DefaultTagger('H')\n",
      "tagger1 = nltk.tag.UnigramTagger(tsents, backoff=tagger0)\n",
      "tagger2 = nltk.tag.BigramTagger(tsents, backoff=tagger1)\n",
      "tagger3 = nltk.tag.TrigramTagger(tsents, backoff=tagger2)\n",
      "tagger3.tag(nltk.word_tokenize(\"Esta frase do Renato tem que ser bem classificada.\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 98,
       "text": [
        "[('Esta', 'H'),\n",
        " ('frase', u'N'),\n",
        " ('do', u'N'),\n",
        " ('Renato', 'H'),\n",
        " ('tem', u'V'),\n",
        " ('que', u'K'),\n",
        " ('ser', u'V'),\n",
        " ('bem', u'A'),\n",
        " ('classificada', u'P'),\n",
        " ('.', u'.')]"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tsents[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 97,
       "text": [
        "[[('um', '>'), ('revivalismo', 'H'), ('refrescante', 'N')],\n",
        " [('o', '>'),\n",
        "  ('7_e_meio', 'H'),\n",
        "  ('\\xe9', 'P'),\n",
        "  ('um', '>'),\n",
        "  ('ex-libris', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('a', '>'),\n",
        "  ('noite', 'H'),\n",
        "  ('algarvia', 'N'),\n",
        "  ('.', '.')],\n",
        " [('\\xc9', 'P'),\n",
        "  ('uma', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('as', '>'),\n",
        "  ('mais', '>'),\n",
        "  ('antigas', 'H'),\n",
        "  ('discotecas', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('algarve', 'H'),\n",
        "  (',', ','),\n",
        "  ('situada', 'P'),\n",
        "  ('em', 'H'),\n",
        "  ('albufeira', 'P'),\n",
        "  (',', ','),\n",
        "  ('que', 'S'),\n",
        "  ('continua', 'A'),\n",
        "  ('a', 'P'),\n",
        "  ('manter', 'M'),\n",
        "  ('os', '>'),\n",
        "  ('tra\\xe7os', 'H'),\n",
        "  ('decorativos', 'N'),\n",
        "  ('e', 'C'),\n",
        "  ('as', '>'),\n",
        "  ('clientelas', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('sempre', 'P'),\n",
        "  ('.', '.')],\n",
        " [('\\xc9', 'P'),\n",
        "  ('um_pouco', 'A'),\n",
        "  ('a', '>'),\n",
        "  ('vers\\xe3o', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('uma', '>'),\n",
        "  ('esp\\xe9cie', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('\\xab', '\\xab'),\n",
        "  ('outro', '>'),\n",
        "  ('lado', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('a', '>'),\n",
        "  ('noite', 'H'),\n",
        "  (',', ','),\n",
        "  ('a', 'H'),\n",
        "  ('meio', '>'),\n",
        "  ('caminho', 'H'),\n",
        "  ('entre', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('devaneios', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('uma', '>'),\n",
        "  ('fauna', 'H'),\n",
        "  ('perif\\xe9rica', 'N'),\n",
        "  (',', ','),\n",
        "  ('seja', 'P'),\n",
        "  ('de', 'H'),\n",
        "  ('lisboa', 'C'),\n",
        "  (',', ','),\n",
        "  ('londres', 'C'),\n",
        "  (',', ','),\n",
        "  ('dublin', 'C'),\n",
        "  ('ou', 'C'),\n",
        "  ('faro', 'C'),\n",
        "  ('e', 'C'),\n",
        "  ('portim\\xe3o', 'C'),\n",
        "  (',', ','),\n",
        "  ('e', 'C'),\n",
        "  ('a', '>'),\n",
        "  ('postura', 'H'),\n",
        "  ('circunspecta', 'N'),\n",
        "  ('de', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('fi\\xe9is', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('a', '>'),\n",
        "  ('casa', 'H'),\n",
        "  (',', ','),\n",
        "  ('que', 'S'),\n",
        "  ('de', 'H'),\n",
        "  ('ela', 'P'),\n",
        "  ('esperam', 'P'),\n",
        "  ('a', '>'),\n",
        "  ('m\\xfasica', 'H'),\n",
        "  ('\\xab', '\\xab'),\n",
        "  ('geracionista', 'N'),\n",
        "  ('de', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('60', 'H'),\n",
        "  ('ou', 'C'),\n",
        "  ('de', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('70', 'H'),\n",
        "  ('.', '.')],\n",
        " [('n\\xe3o', 'A'),\n",
        "  ('deixa', 'A'),\n",
        "  ('de', 'P'),\n",
        "  ('ser', 'M'),\n",
        "  (',', ','),\n",
        "  ('em', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('tempos', 'H'),\n",
        "  ('que', 'S'),\n",
        "  ('correm', 'P'),\n",
        "  (',', ','),\n",
        "  ('um', '>'),\n",
        "  ('certo', '>'),\n",
        "  ('\\xab', '\\xab'),\n",
        "  ('very_typical', 'H'),\n",
        "  ('algarvio', 'N'),\n",
        "  (',', ','),\n",
        "  ('cabe\\xe7a', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('cartaz', 'P'),\n",
        "  ('para', 'H'),\n",
        "  ('os', 'H'),\n",
        "  ('que', 'S'),\n",
        "  ('querem', 'P'),\n",
        "  ('fugir', 'P'),\n",
        "  ('a', 'H'),\n",
        "  ('algumas', '>'),\n",
        "  ('movimenta\\xe7\\xf5es', 'H'),\n",
        "  ('nocturnas', 'N'),\n",
        "  ('j\\xe1', '>'),\n",
        "  ('a', 'H'),\n",
        "  ('caminho', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('a', '>'),\n",
        "  ('ritualiza\\xe7\\xe3o', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('massas', 'P'),\n",
        "  (',', ','),\n",
        "  ('de', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('g\\xe9nero', 'H'),\n",
        "  ('\\xab', '\\xab'),\n",
        "  ('vamos', 'P'),\n",
        "  ('todos', 'S'),\n",
        "  ('a', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('calypso', 'H'),\n",
        "  ('e', 'C'),\n",
        "  ('encontramos-', 'P'),\n",
        "  ('nos', 'A'),\n",
        "  ('em', 'H'),\n",
        "  ('a', '>'),\n",
        "  ('locomia', 'H'),\n",
        "  ('.', '.')],\n",
        " [('e', 'C'),\n",
        "  ('assim', 'A'),\n",
        "  (',', ','),\n",
        "  ('a', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('2,5', '>'),\n",
        "  ('milh\\xf5es', 'H'),\n",
        "  ('que', 'A'),\n",
        "  ('o', '>'),\n",
        "  ('minist\\xe9rio_do_planeamento_e_administra\\xe7\\xe3o_do_territ\\xf3rio', 'H'),\n",
        "  ('j\\xe1', 'A'),\n",
        "  ('gasta', 'P'),\n",
        "  ('em', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('pagamento', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('pessoal', 'H'),\n",
        "  ('afecto', 'P'),\n",
        "  ('a', 'H'),\n",
        "  ('estes', '>'),\n",
        "  ('organismos', 'H'),\n",
        "  (',', ','),\n",
        "  ('v\\xeam', 'P'),\n",
        "  ('juntar-', 'P'),\n",
        "  ('se', 'A'),\n",
        "  ('os', '>'),\n",
        "  ('montantes', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('as', '>'),\n",
        "  ('obras', 'H'),\n",
        "  ('propriamente', '>'),\n",
        "  ('ditas', 'H'),\n",
        "  (',', ','),\n",
        "  ('que', 'A'),\n",
        "  ('os', '>'),\n",
        "  ('munic\\xedpios', 'H'),\n",
        "  (',', ','),\n",
        "  ('j\\xe1', '>'),\n",
        "  ('com', 'H'),\n",
        "  ('projectos', 'H'),\n",
        "  ('em', 'H'),\n",
        "  ('a', '>'),\n",
        "  ('m\\xe3o', 'H'),\n",
        "  (',', ','),\n",
        "  ('v\\xeam', 'A'),\n",
        "  ('reivindicar', 'M'),\n",
        "  ('junto', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('executivo', 'H'),\n",
        "  (',', ','),\n",
        "  ('como', 'H'),\n",
        "  ('salienta', 'P'),\n",
        "  ('aquele', '>'),\n",
        "  ('membro', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('governo', 'H'),\n",
        "  ('.', '.')],\n",
        " [('e', 'C'),\n",
        "  ('o', '>'),\n",
        "  ('dinheiro', 'H'),\n",
        "  ('\\xab', '\\xab'),\n",
        "  ('n\\xe3o', 'A'),\n",
        "  ('falta', 'P'),\n",
        "  ('s\\xf3', '>'),\n",
        "  ('a', 'H'),\n",
        "  ('as', '>'),\n",
        "  ('c\\xe2maras', 'H'),\n",
        "  (',', ','),\n",
        "  ('lembra', 'P'),\n",
        "  ('o', '>'),\n",
        "  ('secret\\xe1rio_de_estado', 'H'),\n",
        "  (',', ','),\n",
        "  ('que', 'S'),\n",
        "  ('considera', 'P'),\n",
        "  ('que', 'S'),\n",
        "  ('a', '>'),\n",
        "  ('solu\\xe7\\xe3o', 'H'),\n",
        "  ('para', 'H'),\n",
        "  ('as', '>'),\n",
        "  ('autarquias', 'H'),\n",
        "  ('\\xe9', 'P'),\n",
        "  ('\\xab', '\\xab'),\n",
        "  ('especializarem-', 'P'),\n",
        "  ('se', 'A'),\n",
        "  ('em', 'H'),\n",
        "  ('fundos', 'H'),\n",
        "  ('comunit\\xe1rios', 'N'),\n",
        "  ('.', '.')],\n",
        " [('mas', 'C'),\n",
        "  ('como', 'A'),\n",
        "  (',', ','),\n",
        "  ('se', 'S'),\n",
        "  ('muitas', 'S'),\n",
        "  ('n\\xe3o', 'A'),\n",
        "  ('disp\\xf5em', 'P'),\n",
        "  (',', ','),\n",
        "  ('em', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('seus', '>'),\n",
        "  ('quadros', 'H'),\n",
        "  (',', ','),\n",
        "  ('de', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('t\\xe9cnicos', 'H'),\n",
        "  ('necess\\xe1rios', 'N'),\n",
        "  ('?', '?')],\n",
        " [('\\xab', '\\xab'),\n",
        "  ('encomendem-', 'P'),\n",
        "  ('nos', 'A'),\n",
        "  ('a', 'H'),\n",
        "  ('projectistas', 'H'),\n",
        "  ('de_fora', 'N'),\n",
        "  ('porque', 'S'),\n",
        "  (',', ','),\n",
        "  ('se', 'S'),\n",
        "  ('as', '>'),\n",
        "  ('obras', 'H'),\n",
        "  ('vierem', 'A'),\n",
        "  ('a', 'P'),\n",
        "  ('ser', 'A'),\n",
        "  ('financiadas', 'M'),\n",
        "  (',', ','),\n",
        "  ('eles', 'S'),\n",
        "  ('at\\xe9', 'A'),\n",
        "  ('saem', 'P'),\n",
        "  ('de_gra\\xe7a', 'A'),\n",
        "  (',', ','),\n",
        "  ('j\\xe1_que', 'S'),\n",
        "  (',', ','),\n",
        "  ('em', 'H'),\n",
        "  ('esse', '>'),\n",
        "  ('caso', 'H'),\n",
        "  (',', ','),\n",
        "  ('\\xab', '\\xab'),\n",
        "  ('os', '>'),\n",
        "  ('fundos', 'H'),\n",
        "  ('comunit\\xe1rios', 'N'),\n",
        "  ('pagam', 'P'),\n",
        "  ('os', '>'),\n",
        "  ('projectos', 'H'),\n",
        "  (',', ','),\n",
        "  ('o', '>'),\n",
        "  ('mesmo', 'H'),\n",
        "  ('n\\xe3o', 'A'),\n",
        "  ('acontecendo', 'P'),\n",
        "  ('quando', 'A'),\n",
        "  ('eles', 'S'),\n",
        "  ('s\\xe3o', 'A'),\n",
        "  ('feitos', 'M'),\n",
        "  ('por', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('gat', 'H'),\n",
        "  (',', ','),\n",
        "  ('dado', 'P'),\n",
        "  ('serem', 'P'),\n",
        "  ('organismos', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('estado', 'H'),\n",
        "  ('.', '.')],\n",
        " [('essa', 'S'),\n",
        "  ('poder\\xe1', 'A'),\n",
        "  ('vir', 'A'),\n",
        "  ('a', 'P'),\n",
        "  ('ser', 'M'),\n",
        "  ('uma', '>'),\n",
        "  ('hip\\xf3tese', 'H'),\n",
        "  (',', ','),\n",
        "  ('at\\xe9', '>'),\n",
        "  ('porque', 'S'),\n",
        "  (',', ','),\n",
        "  ('em', 'H'),\n",
        "  ('o', '>'),\n",
        "  ('terreno', 'H'),\n",
        "  (',', ','),\n",
        "  ('a', '>'),\n",
        "  ('capacidade', 'H'),\n",
        "  ('de', 'H'),\n",
        "  ('os', '>'),\n",
        "  ('gat', 'H'),\n",
        "  ('est\\xe1', 'P'),\n",
        "  ('cada_vez_mais', '>'),\n",
        "  ('enfraquecida', 'H'),\n",
        "  ('.', '.')]]"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}